{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25f58c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Python libraries\n",
    "import pandas as pd                  # Use pandas.DataFrame to manipulate data\n",
    "import matplotlib.pyplot as plt      # Standard plotting library\n",
    "\n",
    "# Data preprocessing\n",
    "from sklearn import preprocessing    # Data preprocessing\n",
    "\n",
    "# Model selection - split data, cv, model evaluation\n",
    "from sklearn.model_selection import train_test_split    # Split dataset into training and test sets\n",
    "from sklearn.model_selection import cross_val_score     # k-fold cross-validation\n",
    "from sklearn.model_selection import GridSearchCV        # search for best parameters\n",
    "from sklearn import metrics                             # metrics to evaluate the model performance\n",
    "from sklearn.metrics import classification_report, confusion_matrix    # analyze prediction made by the classification model\n",
    "\n",
    "# Machine learning algorithms\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Feature Selection\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "\n",
    "# Feature extraction - Decomposition\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import FactorAnalysis\n",
    "\n",
    "# Ensemble learning methods\n",
    "from sklearn.ensemble import BaggingClassifier            # Bagging - (B)ootstrap (AGG)regat(ING)\n",
    "from sklearn.ensemble import AdaBoostClassifier           # Boosting - (ADA)ptive (BOOST)ing\n",
    "from sklearn.ensemble import VotingClassifier             # Voting\n",
    "\n",
    "# Itertools - here, used to generate combinations of base classifiers for voting\n",
    "import itertools\n",
    "# Silence all war\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c51303e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading train data set in dataframe from train_data.csv file\n",
    "df = pd.read_csv(\"C://Users//52816//Desktop//503 project//winequality-white.csv\")\n",
    "                \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82a52ccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\52816\\AppData\\Local\\Temp\\ipykernel_21300\\3025537392.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['quality'].iloc[i] = 1\n",
      "C:\\Users\\52816\\AppData\\Local\\Temp\\ipykernel_21300\\3025537392.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['quality'].iloc[i] = 2\n",
      "C:\\Users\\52816\\AppData\\Local\\Temp\\ipykernel_21300\\3025537392.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['quality'].iloc[i] = 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1    3655\n",
       "2    1060\n",
       "0     183\n",
       "Name: quality, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"C://Users//52816//Desktop//503 project//winequality-white.csv\")\n",
    "for i in range(len(df)):\n",
    "    if df['quality'].iloc[i] == 3 or df['quality'].iloc[i] == 4  :\n",
    "        df['quality'].iloc[i] = 0\n",
    "    if  df['quality'].iloc[i] == 6 or df['quality'].iloc[i] == 5 :\n",
    "        df['quality'].iloc[i] = 1\n",
    "    if df['quality'].iloc[i] == 7 or df['quality'].iloc[i] == 8 or df['quality'].iloc[i] == 9 :\n",
    "        df['quality'].iloc[i] = 2\n",
    "df['quality'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c973df0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.0</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.36</td>\n",
       "      <td>20.7</td>\n",
       "      <td>0.045</td>\n",
       "      <td>45.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>1.001</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.45</td>\n",
       "      <td>8.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "0            7.0              0.27         0.36            20.7      0.045   \n",
       "\n",
       "   free sulfur dioxide  total sulfur dioxide  density   pH  sulphates  alcohol  \n",
       "0                 45.0                 170.0    1.001  3.0       0.45      8.8  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Indicate the target column\n",
    "target = df['quality']\n",
    "# Indicate the columns that will serve as features\n",
    "features = df.drop('quality', axis = 1)\n",
    "features.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045125bb",
   "metadata": {},
   "outputs": [],
   "source": [
    " df['quality']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a2d0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "features = scaler.fit_transform(features)\n",
    "features[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec91d21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef88a67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35c8fc21",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(features, target, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd934e95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec433ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function #1: train and evaluate model performance.\n",
    "# - The parameter `estimator` takes a list of classifier dictionary: {name, classifier}\n",
    "def train_and_evaluate(estimators, X_train, X_test, Y_train, Y_test):\n",
    "    # Nested function #1: Specify performance metric: only \"f1_macro\"\n",
    "    def get_scoring_metric():\n",
    "        return ['f1_macro']\n",
    "    \n",
    "    # Nested function #2: K-fold cross validation\n",
    "    def print_validation_performance(estimator, X_train, Y_train, name=None, cv=10):\n",
    "        for metric in get_scoring_metric():\n",
    "            scores = cross_val_score(estimator, X_train, Y_train, cv=cv, scoring=metric)\n",
    "            estimator_name = \"\"\n",
    "            \n",
    "            if name is not None:\n",
    "                estimator_name = \" {}\".format(name)\n",
    "                \n",
    "            print(\"{} (Validation{}) = \".format(metric, estimator_name), end=\"\")\n",
    "            print(\"{:.4f}\".format(scores.mean()))\n",
    "    \n",
    "    # Nested function #3: Training and testing\n",
    "    def print_test_performance(estimator, X_train, X_test, Y_train, Y_test, name=None):\n",
    "        estimator.fit(X_train, Y_train)\n",
    "        test_predict = estimator.predict(X_test)\n",
    "        \n",
    "        dict_score = {}\n",
    "        \n",
    "        # Get performance score for each metric\n",
    "        for metric in get_scoring_metric():\n",
    "            estimator_name = \"\"\n",
    "            score = 0.0\n",
    "            \n",
    "            if name is not None:\n",
    "                estimator_name = \" {}\".format(name) \n",
    "            \n",
    "            average = None\n",
    "            acc_flag = True\n",
    "            \n",
    "            if \"macro\" in metric:\n",
    "                average = \"macro\"\n",
    "                acc_flag = False\n",
    "            elif \"weighted\" in metric:\n",
    "                average = \"weighted\"\n",
    "                acc_flag = False\n",
    "            \n",
    "            print(\"{} (Test{}) = \".format(metric, estimator_name), end=\"\")\n",
    "            \n",
    "            # Currently only supports accuracy and f1_score.\n",
    "            if acc_flag:\n",
    "                score = metrics.accuracy_score(Y_test, test_predict)\n",
    "            else:\n",
    "                score = metrics.f1_score(Y_test, test_predict, average=average)\n",
    "            \n",
    "            print(\"{:.4f}\".format(score))\n",
    "            \n",
    "            dict_score[metric] = score\n",
    "        \n",
    "        print(confusion_matrix(Y_test, test_predict))         # Confusion matrix\n",
    "        print(classification_report(Y_test, test_predict))    # Classification report\n",
    "        \n",
    "        return dict_score\n",
    "    \n",
    "    dict_est_score = {}\n",
    "    \n",
    "    # Print validation and test performance for all classifiers in the list.\n",
    "    for key_est in estimators:\n",
    "        print_validation_performance(estimators[key_est], X_train, Y_train, name=key_est)\n",
    "        dict_est_score[key_est] = print_test_performance(estimators[key_est], \n",
    "                                                         X_train, X_test, Y_train, Y_test, \n",
    "                                                         name=key_est)\n",
    "        print()\n",
    "        \n",
    "    return dict_est_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e588a0",
   "metadata": {},
   "source": [
    "# 3.集成学习\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6efea6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_clf_default = {\n",
    "    \"K-Nearest Neighbors\": KNeighborsClassifier(), \n",
    "    \"Decision Tree\": DecisionTreeClassifier(random_state=0),\n",
    "    \"Linear SVM\": SVC(kernel='linear', max_iter=1500), \n",
    "    \"Polynomial SVM\": SVC(kernel='poly', max_iter=1500), \n",
    "    \"RBF SVM\": SVC(kernel='rbf', max_iter=1500), \n",
    "    \"Sigmoid SVM\": SVC(kernel='sigmoid', max_iter=1500),\n",
    "    \"Logistic Regression\": LogisticRegression()\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d27c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify one classifier\n",
    "clf_index = 1\n",
    "\n",
    "estimator_name = list(dict_clf_default.keys())[clf_index]\n",
    "estimator = {estimator_name: dict_clf_default[estimator_name]}\n",
    "\n",
    "train_and_evaluate(estimator, x_train, x_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb727d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Baseline - compare all classifiers\n",
    "list_clf = []\n",
    "list_score = []\n",
    "\n",
    "score_param = \"f1_score\"\n",
    "average_param = \"macro\"\n",
    "\n",
    "# Generating results\n",
    "# ---\n",
    "# Iterate through all classifiers\n",
    "for clf in dict_clf_default:\n",
    "    estimator = { clf: dict_clf_default[clf] }\n",
    "    try:\n",
    "        score = train_and_evaluate(estimator, x_train, x_test, y_train, y_test)\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_class_name = e.__class__.__name__\n",
    "        print(\"{}: {}\\n\".format(error_class_name, e))\n",
    "        continue\n",
    "        \n",
    "    print()\n",
    "    \n",
    "    list_clf.append(clf)\n",
    "    list_score = list_score + list(score[clf].values())\n",
    "\n",
    "    \n",
    "# Create data frame\n",
    "# ---\n",
    "data = dict(classifier=list_clf, test_performance=list_score)\n",
    "df_performance = pd.DataFrame(data=data)\n",
    "\n",
    "# # Save and export df_performance to CSV file (optional)\n",
    "# df_performance.to_csv(r'final_project_performance_default.csv', index = False, hea\n",
    "\n",
    "# Plotting\n",
    "# ---\n",
    "str_x_label = \"Model Test Performance ({} {})\".format(average_param.capitalize(), score_param.capitalize())\n",
    "font_size = 12\n",
    "\n",
    "plt.figure(figsize=(9, 6))\n",
    "plt.barh(list_clf, list_score)\n",
    "\n",
    "plt.title(\"{} on Different Classifiers\".format(str_x_label), fontsize=(font_size + 4))\n",
    "plt.xlabel(str_x_label, fontsize=(font_size + 2))\n",
    "plt.ylabel(\"Classifiers\", fontsize=(font_size + 2))\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# # Save plot into PNG (optional) \n",
    "# plt.savefig(\"final_project_performance_default.png\", dpi=300, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0012ee15",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2515efe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "680ff4b8",
   "metadata": {},
   "source": [
    "# 3.1Bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309af483",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify classifier\n",
    "clf_index = 1    # Accepts value from 0 - 6\n",
    "\n",
    "# Define parameter values for BaggingClassifier object\n",
    "base = list(dict_clf_default.values())[clf_index]\n",
    "bagging_param = dict(base_estimator=base,\n",
    "                     n_estimators=100,    # create 100 different models using the same `base_estimator`\n",
    "                     random_state=0)\n",
    "\n",
    "# Define BaggingClassifier object\n",
    "model_bagging = BaggingClassifier(**bagging_param)\n",
    "\n",
    "estimator_name = \"Bagging - {}\".format(list(dict_clf_default.keys())[clf_index])\n",
    "estimator = {estimator_name: model_bagging}\n",
    "\n",
    "# Train and evaluate the performance of the bagging classifier\n",
    "train_and_evaluate(estimator, x_train, x_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc6793a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bagging - compare all classifiers\n",
    "list_clf = []\n",
    "list_score = []\n",
    "\n",
    "score_param = \"f1_score\"\n",
    "average_param = \"macro\"\n",
    "\n",
    "bagging_param = dict(n_estimators=100, random_state=0)\n",
    "\n",
    "\n",
    "# Generate results\n",
    "# ---\n",
    "# Iterate through all classifiers\n",
    "for i, clf in enumerate(dict_clf_default):\n",
    "    base = dict_clf_default[clf]\n",
    "    \n",
    "    params = bagging_param.copy()\n",
    "    params[\"base_estimator\"] = base\n",
    "\n",
    "    model_bagging = BaggingClassifier(**params)\n",
    "    \n",
    "    estimator = { clf: model_bagging }\n",
    "    \n",
    "    try:\n",
    "        score = train_and_evaluate(estimator, x_train, x_test, y_train, y_test)\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_class_name = e.__class__.__name__\n",
    "        print(\"{}: {}\\n\".format(error_class_name, e))\n",
    "        continue\n",
    "    \n",
    "    print()\n",
    "    \n",
    "    list_clf.append(clf)\n",
    "    list_score = list_score + list(score[clf].values())\n",
    "    # Create data frame\n",
    "# ---\n",
    "data = dict(classifier=list_clf, test_performance=list_score)\n",
    "df_performance = pd.DataFrame(data=data)\n",
    "\n",
    "# # Save and export df_performance to CSV file (optional)\n",
    "# df_performance.to_csv(r'final_project_performance_bagging.csv', index = False, header = True)\n",
    "\n",
    "\n",
    "# Plotting\n",
    "# ---\n",
    "str_x_label = \"Model Test Performance ({} {})\".format(average_param.capitalize(), score_param.capitalize())\n",
    "font_size = 12\n",
    "\n",
    "plt.figure(figsize=(9, 6))\n",
    "plt.barh(list_clf, list_score)\n",
    "\n",
    "plt.title(\"Bagging: {} on Different Base Classifiers\".format(str_x_label), fontsize=(font_size + 4))\n",
    "plt.xlabel(str_x_label, fontsize=(font_size + 2))\n",
    "plt.ylabel(\"Base Classifiers\", fontsize=(font_size + 2))\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# # Save plot into PNG (optional) \n",
    "# plt.savefig(\"final_project_performance_bagging.png\", dpi=300, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a270bd13",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "features = scaler.fit_transform(features)\n",
    "features[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938e495c",
   "metadata": {},
   "source": [
    "# 3.2 boosting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd85faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "_ = [print(\"Index: {} - {}\".format(i, clf)) for i, clf in enumerate(dict_clf_default)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052960e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify one classifier\n",
    "clf_index = 1    # Accepts value from 0 - 6\n",
    "\n",
    "# Define parameter values for AdaBoostClassifier object\n",
    "base = list(dict_clf_default.values())[clf_index]\n",
    "boosting_param = dict(base_estimator=base,\n",
    "                     n_estimators=100,\n",
    "                     random_state=0)\n",
    "\n",
    "# Define AdaBoostClassifier object\n",
    "model_boosting = AdaBoostClassifier(**boosting_param)\n",
    "\n",
    "estimator_name = \"Boosting - {}\".format(list(dict_clf_default.keys())[clf_index])\n",
    "estimator = {estimator_name: model_boosting}\n",
    "\n",
    "# Train and evaluate the performance of the AdaBoost classifier\n",
    "train_and_evaluate(estimator, x_train, x_test, y_train, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb20cdd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all classifiers\n",
    "list_clf = []\n",
    "list_score = []\n",
    "\n",
    "score_param = \"f1_score\"\n",
    "average_param = \"macro\"\n",
    "\n",
    "feature_train = x_train\n",
    "feature_test = x_test\n",
    "\n",
    "# Initialize parameter values of boosting classifier\n",
    "boosting_param = dict(n_estimators=100, random_state=0)\n",
    "\n",
    "# Loop through all classifiers\n",
    "# ---\n",
    "for i, clf in enumerate(dict_clf_default):\n",
    "    print(\"- {}\".format(clf))\n",
    "    base = dict_clf_default[clf]\n",
    "    \n",
    "    params = boosting_param.copy()\n",
    "    params[\"base_estimator\"] = base\n",
    "    \n",
    "    model_boosting = AdaBoostClassifier(**params)\n",
    "    \n",
    "    estimator = { clf: model_boosting }\n",
    "    \n",
    "    try:\n",
    "        score = train_and_evaluate(estimator, x_train, x_test, y_train, y_test)\n",
    "    \n",
    "    except Exception as e:\n",
    "        error_class_name = e.__class__.__name__\n",
    "        print(\"{}: {}\\n\".format(error_class_name, e))\n",
    "        continue\n",
    "    \n",
    "    print()\n",
    "    \n",
    "    list_clf.append(clf)\n",
    "    list_score = list_score + list(score[clf].values())\n",
    "    # Create data frame\n",
    "# ---\n",
    "data = dict(classifier=list_clf, test_performance=list_score)\n",
    "df_performance = pd.DataFrame(data=data)\n",
    "\n",
    "# # Save and export df_performance to CSV file\n",
    "# df_performance.to_csv(r'final_project_performance_boosting.csv', index = False, header = True)\n",
    "\n",
    "# Plotting\n",
    "# ---\n",
    "str_x_label = \"Model Test Performance ({} {})\".format(average_param.capitalize(), score_param.capitalize())\n",
    "font_size = 12\n",
    "\n",
    "plt.figure(figsize=(9, 6))\n",
    "plt.barh(list_clf, list_score)\n",
    "\n",
    "plt.title(\"Boosting: {} on Different Base Classifiers\".format(str_x_label), fontsize=(font_size + 4))\n",
    "plt.xlabel(str_x_label, fontsize=(font_size + 2))\n",
    "plt.ylabel(\"Base Classifiers\", fontsize=(font_size + 2))\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# # Save plot into PNG (optional) \n",
    "# plt.savefig(\"final_project_performance_boosting.png\", dpi=300, bbox_inches=\"tight\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c423a8",
   "metadata": {},
   "source": [
    "# 3.3 Voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ea09a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_param = \"f1_score\"\n",
    "average_param = \"macro\"\n",
    "\n",
    "# specify number of base classifiers\n",
    "num_base_clf = 3                                # Accepts values from 2 - 4 (number of base classifiers)\n",
    "list_clf = [[] for i in range(num_base_clf)]    # 2-dimensional list, to store list of classifiers in\n",
    "                                                # each voting classifier\n",
    "list_score = []\n",
    "\n",
    "# Generate all possible combinations of base classifiers with # of classifiers = num_base_clf\n",
    "# ---\n",
    "# For example: if num_base_clf = 3, \n",
    "# [A, B, C, D] -> [A, B, C], [A, B, D], [A, C, D] and [B, C, D]\n",
    "combinations_base_clf = itertools.combinations(dict_clf_default, num_base_clf)\n",
    "\n",
    "# Iterate through all generated lists\n",
    "for comb in combinations_base_clf:\n",
    "    list_base_clf = []\n",
    "    \n",
    "    # Return number of SVM classifiers in a list\n",
    "    count_svm = sum(\"SVM\" in clf for clf in comb)    \n",
    "    \n",
    "    # At most one SVM classifier in the list.\n",
    "    # - skip all the lists with >1 classifiers - speed up iteration\n",
    "    # - ensure variation in machine learning algorithms in the voting classifier\n",
    "    if count_svm <= 1:\n",
    "        for i, clf in enumerate(comb):\n",
    "            list_base_clf.append((clf, dict_clf_default[clf]))\n",
    "            list_clf[i].append(clf)\n",
    "    \n",
    "        print(comb)\n",
    "        print(\"---\")\n",
    "        \n",
    "        try:\n",
    "            model_voting = VotingClassifier(estimators=list_base_clf)\n",
    "            estimator_name = \"Voting Classifier\"\n",
    "            estimator = { estimator_name : model_voting }\n",
    "            score = train_and_evaluate(estimator, x_train, x_test, y_train, y_test)\n",
    "            \n",
    "            list_score = list_score + list(score[estimator_name].values())\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_class_name = e.__class__.__name__\n",
    "            print(\"{}: {}\\n\".format(error_class_name, e))\n",
    "            continue\n",
    "        \n",
    "        print()\n",
    "\n",
    "        \n",
    "# Create data frame\n",
    "# ---\n",
    "data = dict()\n",
    "for i, base_clf in enumerate(list_clf):\n",
    "    clf_key = \"classifier_{}\".format(i)\n",
    "    data[clf_key] = base_clf\n",
    "\n",
    "data[\"test_performance\"] = list_score\n",
    "df_performance = pd.DataFrame(data=data)\n",
    "\n",
    "# Save and export df_performance to CSV file (optional)\n",
    "df_performance.to_csv(r'final_project_performance_voting_{}clf.csv'.format(num_base_clf), index = False, header = True)\n",
    "\n",
    "df_performance\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83eb36f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd52b624",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291a5c5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c198f771",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c52001",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b256b0a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
